---
title: "Section 7: Data Analysis in Depth"
subtitle: "Springboard: Foundations of Data Science"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
    number_sections: true
    theme: paper
    highlight: pygments
    df_print: paged
author: "Antoine Beauchamp"
date: "February 10th, 2017"
---

<!-- 
Some great themes: 
- Cerulean
- Flatly
- Cosmo
- paper

Some code highlights:
- Espresso
- tango
- pygments
- haddock
-->

#Linear Regression

## The Statistical Sommelier: An Intro to Linear Regression

### Predicting the quality of wine

Source: https://www.youtube.com/watch?v=vI3envXmyDs

In this section, we will apply linear regression to **predict the quality/prices of wines**. 

Bordeaux wines have large differences in prices and quality from year to year. Wine is meant to be aged so it’s difficult to tell if wine will be good when it hits the market. Expert tasters predict which wines will be good. 

Can we use analytics to model this process? 

In 1990, Dr. Ashenfelter used linear regression to predict wine quality. 

**Linear regression predicts an outcome, or dependent variable, based on a set of independent variables**. 

In this case, the dependent variable was the typical price in 1990-1991 wine auctions (which is used to approximate quality).

The independent variables were: age (older means more expensive), weather (average growing season temp., harvest rain, winter rain)

### One-variable Linear Regression

Source: https://www.youtube.com/watch?v=ZR_9sOun364

One-variable linear regression just uses one independent variable to predict to dependent variable. 
The goal of linear regression is to create a predictive line through the data.  

Given a set of data, there are a number of model lines that we can draw through the data. Linear regression allows us to find the optimal line to describe the data. 

In general, a one variable linear regression model is a linear equation that predicts the dependent variable, $y$:

$$y^i = B_0 + B_1x^i + e^i$$

where  
  
* $y^i$: Dependent variable for the $i$th observation (data) 
* $x^i$: Independent variable for the $i$th observation (data)
* $e^i$: Error term for the ith observation
* $B_0$: Intercept
* $B_1$: Regression coefficient for the independent variable (slope)

The error term describes the difference between each data point and the mathematical line, i.e.  

$$e^i = y^i - y(x^i)$$

where $y(x_i)$ describes the linear function. 

Our best choices of model coefficients, $B_0$, $B_1$ has the smallest errors (also called **residuals**). 

One measure for the quality of the regression line is the **sum of squared errors (SSE)**. 
This is the sum of the squared residuals, or error terms:

$$SSE = \sum_{i=1}^{N} (e^i)^2 = \sum_{i=1}^{N} (y^i - y(x^i))^2$$

The line $y(x^i)$ that **minimizes SSE** is the **regression model line**. 

SSE is hard to interpret since it scales with $N$ and the units are difficult to understand. 

A better measure is the **Root-Mean-Square Error (RMSE)**: 

$$RMSE = \sqrt{\frac{SSE}{N}}$$

RMSE is normalized by N and is in the same units as the dependent variable. 

Another common measure for a linear regression is $R^2$. 
It compares the best model to a **baseline model**, which is **the model that doesn’t use any variables**. The baseline model **predicts the average value of the dependent variable**, regardless of the independent variables (just a flat line at $y = y_{avg}$ on the graph). 

The sum of squared errors for the baseline model is often known as the **total sum of squares, or SST**.

Then, 


$$R^2 = 1 - \frac{SSE}{SST}$$


* $R^2$ captures the value added from using a model over just predicting the average outcome. 
* $R^2 = 0$ means no improvement over the mean baseline. 
* $R^2 = 1$ means a perfect predictive model (all points lie on the line)

$R^2$ is unitless and universally interpretable, but it can still be hard to compare between problems. Good models for easy problems will have $R^2 \approx 1$. Good models for hard problems can still have $R^2 \approx 0$. 


### Multiple Linear Regression

Source: https://www.youtube.com/watch?v=zthosZpYBUs

So far we have only worked with one independent variable. 

For wine prices, we can use a number of potential variables:  
 
* Average growing season temperature (AGST)
* Harvest rain
* Winter rain
* Age of wine in 1990
* Population of France

We can begin by using each variable in a one-variable regression model. 

AGST gives the best results, with $R^2 = 0.44$, followed by Harvest rain with $R^2 = 0.32$. The rest decrease. 

Multiple linear regression allows us to use all of these variables to improve our predictive ability. 

The multiple linear regression model with $k$ independent variables: 

$$y^i = B_0 + B_1x_1^i + B_2x_2^i + … + B_kx_k^i + e^i$$


The different $x_k^i$ are the different data independent variable data. The “$i$” simply represent individual data points. 

We find the following results: 

* AGST: $R^2=0.44$
* AGST, harvest rain: $R^2=0.71$
* AGST, harvest rain, age: $R^2 = 0.79$
* AGST, harvest rain, age, winter rain: $R^2=0.83$
* AGST, harvest rain, age, winter rain, population: $R^2=0.83$

We see that adding more variables can improve the model, but there are diminishing returns as more variables are added. Not all available variables should be used. Each new variable requires more data. This causes overfitting: high $R^2$ on data used to create the model, but bad performance on unseen, new data. 


### Linear Regression in R

Source: https://www.youtube.com/watch?v=5lcztJ9K42o


Let's build these regression models in R. 

```{r}
library(readr)

wine <- read_csv("wine.csv")
str(wine)
summary(wine)

#One variable linear regression
model1 <- lm(Price ~ AGST, data=wine)
summary(model1)
```

Under "Coefficients"", we can see the intercept estimate and the slope estimate. 

The Multiple R-squared is the standard $R^2$ that was discussed.   

The adjusted R-squared accounts for the number of independent variables used. 

```{r}
# The residuals (errors) for the model are stored in the following vector
model1$residuals

#Compute the SSE
SSE <- sum(model1$residuals^2)
SSE

# Let's make a multiple regression
model2 <- lm(Price ~ AGST + HarvestRain, data=wine)
summary(model2)


SSE <- sum(model2$residuals^2)
SSE
#Much better
```
The value for $R^2$ is higher so this has helped. Adjusted $R^2$ also increased so this is good. 

```{r}
#Let's use all our ind variables
model3 <- lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)
summary(model3)
# R squared and Adjusted R squared better now. 
SSE <- sum(model3$residuals^2)
SSE
```


### Understanding the Model

Source: https://www.youtube.com/watch?v=iTId8PCcqe8


On reading the output of `summary(lm(...))`: 

The "Estimates" column gives coefficients for the model. 

Remaining columns help us determine if the variable should remain in the model, or if its coefficient is **significantly different from 0**. 

A coefficient of 0 means that value of the independent variable doesn't change our prediction for the dependent variable. 

**If a coefficient is not significantly different from 0, we should remove it from the model**
. 
The standard error (`Std. Error`) gives a measure of how much the coefficient is likely to vary from the estimate. 

The **t value** is the estimate/std. error. The larger the absolute value of the t statistic, the more likely the coefficient is to be significant. 
The last column gives the **probability that a coefficient is actually 0**. It will be large if $|t|$ is small, and vice versa. 

R tells us which variables are significant by **using the asterisks at the end of each row**. Three stars is the highest level of significance and corresponds to probabilities < 0.001. This is explained at the bottom of the coefficient output:

* *** means between 0 and 0.001
* ** means 0.001 and 0.01
* \* means between 0.01 and 0.05
* \. means between 0.05 and 0.1 (almost significant)
* Nothing at the end of a row means that the variable is not significant in a model. 

Can we improve our model? 

```{r}
summary(model3)
#Age and francepop are not significant. Let's start by just removing francepop. 
model4 <- lm(Price~AGST+HarvestRain+WinterRain+Age, data=wine)
summary(model4)
#Our adjusted R squared increased, so this model is as good or better than before. 
# We can also see that age is now significant in this model. 
# Why did this happen? Multi-collinearity. 
```

Our adjusted R-squared increased, so this model is as good or better than before. We can also see that age is now significant in this model. 

Why did this happen? It's because of **multi-collinearity**. 

**Correlation** measures the linear relationship between variables:   

* 1 : perfect positive linear relationship
* 0 : no linear relationship
* -1: perfect negative linear relationship

The correlation between FrancePop and Age is actually -0.99. This is very strong. 

```{r}
# Use cor() to find correlation between vars. 
cor(wine$WinterRain, wine$Price)
cor(wine$Age, wine$FrancePop)
```
To generate a full correlation matrix, use cor() on the data set:
```{r}
cor(wine) # Correlation matrixs
```

**Multi-collinearity refers to the situation where two independent variables are highly correlated**. A high correlation between the ind. var. and a dep. var. is a good thing, since this is what we are trying to predict. **Multi-collinearity only applies to two independent variables**. 
Because of multi-collinearity, **we always want to remove the insignificant variables one at a time**. 

What would have happened if we'd remove both age and pop. at the same time? 

```{r}
model5 <- lm(Price ~ AGST + HarvestRain + WinterRain, data=wine)
summary(model5)
```

$R^2$ has dropped in this case because we removed the age variable. 

Why didn't we keep FrancePop instead of Age? We expect Age to have an effect on price but not the population of France. 

To determine our best model, look at the correlation matrix to see if we have any more independent variables with strong correlations. 

This doesn't seem to be the case, so `model4` is our best model. 

### Making Predictions

Source: https://www.youtube.com/watch?v=Ku6CGFgba_s

Our wine model had $R^2=0.83$. 
Tells us our accuracy on the data that we used to build the model. 
But how well does the model perform on new data?

The data used to build the model is often called the training data. 
The new data is called the test data. 

The accuracy of the model on the test data is referred to as out-of-sample accuracy. 

Let’s see how well model performs on test data in R. 

We omitted two data points when building our model. Let's load these. 
```{r}
wineTest <- read_csv("wine_test.csv")
str(wineTest)
#To make predictions on these new test points, will use the function predict()
predictTest <- predict(model4,newdata=wineTest)
predictTest
# For first data point, predict 6.768.., and for second data point predict 6.6849...
#Looking at the str() output, we can see that the actual points are 6.95 and 6.5
#Our predictions seem decent. 
#Let's compute the R^2 for our test data. 
SSE = sum((wineTest$Price-predictTest)^2)
SST = sum((wineTest$Price - mean(wine$Price))^2)
1 - SSE/SST
# The out of sample R^2 is 0.7944. This is pretty for out of sample. Keep in mind that our test data is very small. We should increase this to be more confident about our model. 
```


A better **model** $R^2$ does not necessarily mean better **test set** $R^2$. We need more data to be conclusive. We are looking for something that does well in sample and out of sample. 
The out of sample $R^2$ can be negative because the model can do worse than the base model on the test data. 


### Comparing the Model to the Experts

Source: https://www.youtube.com/watch?v=zGr0OlM8LyY


A linear regression model with only a few variables can predict wine prices well. In many cases, it outperformed wine experts’ opinions. 
This was a quantitative approach to a traditionally qualitative problem. 



## Moneyball: The Power of Sports Analytics


### The Story of Moneyball

Source: https://www.youtube.com/watch?v=Ioio2KtJW34

Moneyball discusses how sports analytics changed baseball. 

The problem: 
Rich teams can afford all-star players
How do the poor teams compete? 

The Oakland A’s won about the same number of games as the Red Sox from 1998-2001, but spent 30M dollars per year rather than 80M that the Red Sox spent. 

Rich teams have four times the payroll of poor teams. 

The A’s can’t afford all stars but they still made it to the playoffs. How? 

They took a quantitative approach to find undervalued players. 

The traditional way of selecitng players was through scouting. 
Scouts would watch high school and college players and report back about skills and athletic build. 

The A’s selected players based on statistics, not their looks

Billy Beane, the A’s manager succeeded in using analytics. Understood the importance of statistics. Hired an analyst, Paul DePodesta. 
His analysus suggested that some skills were undervalued and some skills were overvalued. 
If they could detect the undervalued skills, they could find players at a bargain. 


### Making it to the Playoffs

Source: https://www.youtube.com/watch?v=jDpLyLll7_Y

The goal of a baseball team is to make the playoffs. 

Let’s start by figuring out how many games a team needs to win to make the playoffs, and how many more runs a team needs to score to win that many games. 

Paul DePodesta founds 95 wins for A’s to make it to playoffs. 

How does a team wine games? They score more runs than their opponent. BUt how many more? A’s calculated that they needed to score 135 more runs that they allowed during the regular season. 

```{r}
#Load baseball data
baseball <- read_csv("baseball.csv")
str(baseball)
#RS:Run scored
#RA: Run allowed
#W: Wins

#Start by subsetting data to only include years before 2002. 
moneyball=subset(baseball, Year <2002)
str(moneyball)

#Want to build linear regression to predict wins using difference between runs scored and runs allowed
moneyball$RD = moneyball$RS - moneyball$RA

WinsReg <- lm(W ~ RD, data=moneyball)
summary(WinsReg)
#Strong model to predict wins

#Can use this model to confirm claim that team needs to score at least 135 more runs than they allow in order to win at least 95 games. 
# W = 80.0014 + 0.10577*RD >= 95
# Find RD >= 133.4
# This is very close to the claim made in moneyball. 
```


### Predicting Runs

Source: https://www.youtube.com/watch?v=4MRoSNqO-es

We now need to know how many runs a team will score, and how many runs a team will allow. 
Let's build a linear regression model to predict runs scored. 

The A's discovered that two baseball stats were significantly more important than anything else:
- On base percentage (OPB): Percentage of time a player gets on base 
- Slugging percentage (SLG): How far a player gets around the bases on his turn

Most teams focussed on batting average (BA): getting on base by hitting a ball. 
A's claimed that OBP was most important and that BA was overvalued. 

```{r}
str(moneyball)
#We have OBP, SLG, and BA. Let's use linear regression to predict RS. 
RunsReg <- lm(RS ~ OBP + SLG + BA, data=moneyball)
summary(RunsReg)
#Everything is significant and R^2 = 0.93
#Coefficient for BA is negative. This says that all other things being equal, a team with a higher batting average will score fewer runs. This is counterintuitive. 
# We have a case of multi-collinearity. Let's remove BA. 
RunsReg <- lm(RS ~ OBP + SLG, data=moneyball)
summary(RunsReg)
#This is much better because the model is simpler. The R^2 is about the same. 
# We can find that if we took out OBP or SLG instead of BA, R^2 would ahve decreased more. 
# We can also see that OPB is more significant than SLG because the slope is higher and the variables are on about the same scale. 
```

These reuslts allow us to very the claims from moneyball. 

We can create a similar model for runs allows using
- Opponents on-base percentage (OOBP)
- Opponents slugging percentage (OSLG)

We find the model
RA = -837.38 + 2913.60(OOBP) + 1514.29(OSLG)
with 
R^2 = 0.91
Both vars are significant. 

### Using the model to make predictions 

Source: https://www.youtube.com/watch?v=5OLTKEW0Vbg

Can we predict how many games the 2002 Oakland A's will win using our models?
The models for runs use team statistics. Each year, a baseball team is different. So we don't know the stats. 
But we can estimate them using past player performance. Assumes past performance correlates with future performance and assumes few injuries. 
We can estimate stats for 2002 using 2001 stats. 

At beginning of 2002 season, A's had 24 batters on their rosters. 
Using 2001 stats, estimate
- Team OBP is 0.339
- Team SLG is 0.430

Using our regression model, we find RS = 805

Similarly
- Team OOBP = 0.307
- Team OOSLG = 0.373
Which gives
RA = 622

This gives 
Wins = 100

The predictions closely matched the actual performance in 2002. 
The actual values are:
RS = 800
RA = 653
Wins = 103

The A's set a league record by winning 20 games in a row. Made it to playoffs. 

### Winning the world series

Source: https://www.youtube.com/watch?v=bVxW6tIPE78

Billy Bean and Paul Depodesta see their job as making sure the team makes it to the playoffs. After that all bets are off. 
The A's made it to the playoffs in 2000, 2001, 2002, 2003 but they didn't win the world series. 
Why? 
"Over a long season the luck evens out and the skill shines through. But in a series of three out of fie, or even four out of seven, anything can happen."
The playoffs suffer from the sample size problem. There aren't enough games to make statistical claims. 

Using data 1994-2001 (8 teams in the playoffs)
The correlation between winning the world series and regular season wins is 0.03. 
Winning regular season games gets you to the playoffs, but int he playoffs there are too few games for luck to even out. 
Logistic regression can be used to predict whether a team will win the world series.



### The analytics edge in sports

Source: https://www.youtube.com/watch?v=N29Nf093Nwk

Moneyball discusess:
How it is easier to predict professional success of college players than high school players
Stealing bases, sacrifice bunting, and sacrifice flies are overrated
Pitching stats do no accurately measure pitcher ability. Pitches only control strikeouts, home runs, and walks

Before moneyball techniques became known, in2002 , the A’s were an outlier. They had 20 more wins than teams with equivalent payrolls. As many wins as teams with more than double their payroll. 

Now the A’s are still an efficient team, but they only have 10 more wins than teams with equivalent payroll. Fewer inefficiences. 

Sabermetrics is a more general term for Moneyball techniques. There has been a lot of work done in the field. 

Every major league baseball team now has a stats group. Analytics are also used in other sports. 


Models allow managers to more accurately value players and minimize risk. Relatively simple models can be useful. 


##  Playing Moneyball in the NBA

### NBA Data

Source: https://www.youtube.com/watch?v=WfaKNYR2vAA

```{r}
library(readr)
NBA <- read.csv("NBA_train.csv")
str(NBA)
```
SeasonEnd: Year season ended
Playoffs: Binary for whether team made it to playoffs that year
W: Season wins
PTS: Points scored in regular season
oppPTS: Opponents scored in regular season

The variables with an A mean "attempted". Without an A means it was successful. 

FG: Field goals
X2P: 2 pointers
X3P: 3 pointers
FT: Free throws

Note, R doesn't like when var names begin with a number. This is why there is an X in front of 2P and 3P. 



### Playoffs & Wins

Source: https://www.youtube.com/watch?v=NjnQI6R-fQU

Goal of basektball team: making playoffs. 

How many games does a team need to win to make playoffs? 

```{r}
table(NBA$W, NBA$Playoffs)
```

Looking at the data, we can see that a good threshold for making it to the playoffs is around 42 wins. This gives a good chance of making it to the playoffs. 

Can we use difference between points scored and points allowed ot predict number of games that a team will win? 

```{r}
NBA$PTSdiff = NBA$PTS - NBA$oppPTS
plot(NBA$PTSdiff, NBA$W)
```


Suggests a very strong relationship. Linear regression will be a good tool to use. 

```{r}
WinsReg <- lm(W ~ PTSdiff, data=NBA)
summary(WinsReg)
```

Strong R^2 and very significant variable. Verifies the scatterplot. 

We have:
$W = 41 + 0.0326PTSdiff$

What PTSdiff do we want to get 42? 

Find PTSdiff >= 30.67



### Points scored

Source: https://www.youtube.com/watch?v=6pcIGhSaSWw

Let's build an equation to predict points scored using basketball stats. 
Dep. var: PTS
Ind. var. would be common stats, listed above. 

```{r}
PointsReg <- lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + TOV + STL + BLK, data=NBA)
summary(PointsReg)
```

Some vars very significant. Others less significant. Decent R^2 value, so there definitely is a linear relationship. 

```{r}
(SSE = sum(PointsReg$residuals^2))
```
This is very large, but remember this isn't interpretable. Compute the RMSE instead. 

```{r}
(RMSE = sqrt(SSE/nrow(NBA)))
```

This seems like a lot, but the average number of poitns in a season is

```{r}
mean(NBA$PTS)
```

So it isn't so bad. 

But we can still improve our model. Let's try to remove some vars one at a time. 

Let's start with turnover, TOV. The p-value for the TOV variable was the highest, so it is the least significant. 

```{r}
PointsReg2 <- lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + STL + BLK, data=NBA)
summary(PointsReg2)
```

Our R^2 is about the same. Goes down very slightly. So we are justified. Next let's remobe DRB. 

```{r}
PointsReg3 <- lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + STL + BLK, data=NBA)
summary(PointsReg3)
```

Again no change in R^2, so we are justified. 
Now try removing BLK. 

```{r}
PointsReg4 <- lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + STL, data=NBA)
summary(PointsReg4)
```
Same R^2. 

This model is a bit simpler. Still have a good R^2. Let's look at SSE and RMSE again. 

Recall
```{r}
SSE
RMSE
```
Now, 
```{r}
(SSE_4 = sum(PointsReg4$residuals^2))
(RMSE = sqrt(SSE_4/nrow(NBA)))
```

We see that removing the variables has not significantly increased the error in our model. This is good. 


### Making predictions

Source: https://www.youtube.com/watch?v=JNpslqYwDs4

Let's make predictions for the 2012-2013 season, using our test data set. 

```{r}
NBA_test <- read.csv("NBA_test.csv")
PointsPredictions <- predict(PointsReg4, newdata = NBA_test)
```

Let's compute the out-of-sample R^2 to see how well this model works. We had a good in-sample R^2, which is about hte training data. 

```{r}
SSE = sum((PointsPredictions - NBA_test$PTS)^2)
SST = sum(((mean(NBA$PTS)-NBA_test$PTS))^2)
(R2 = 1 - SSE/SST)
(RMSE = sqrt(SSE/nrow(NBA_test)))
```

This isn't too bad. 


#Logistic Regression

##Modeling the Expert

##The Framingham Heart Study: Evaluating Risk Factors to Save Lives

##Election Forecasting: Predicting the Winners Before Any Votes Are Cast

#Clustering

## Introduction

##Recommendations Worth a Million: Intro to Clustering

##Predictive Diagnosis: Discovering Patterns for Disease Recognition

##Seeing the Big Picture: Segmenting Images to Create Data

#Model Evaluation: Concepts

#Cross-validation in R

#Trees

##Judge, Jury and the Classifier: Intro to Trees

##Keeping an Eye on Healthcare Costs

##Location, Location, Location: Regressions Trees for Housing Data

#Text Analytics

##Introduction

##Turning Tweets into Knowledge

##Man vs. Machine: How IBM Built a Jeopardy Champion


##Predictive Coding: Bring Analytics into the Courtroom

